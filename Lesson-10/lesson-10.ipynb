{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从零开始实现神经网络框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data['data']\n",
    "labels = data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    计算图节点类基类\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputs=None, name=''):\n",
    "        if inputs is None:\n",
    "            inputs = []\n",
    "        self.inputs = inputs\n",
    "        self.name = name\n",
    "        self.value = None\n",
    "        self.outputs = []\n",
    "        self.gradients = {}  # 梯度\n",
    "        self.graph = default_graph  # 计算图对象，默认为全局对象default_graph\n",
    "        self.params = []\n",
    "\n",
    "        for node in self.inputs:\n",
    "            node.outputs.append(self)  # 建立节点之间的连接\n",
    "\n",
    "        # 将本节点添加到计算图中\n",
    "        self.graph.add_node(self)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        前向传播计算本节点的值\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        反向传播，计算结果节点对本节点的梯度\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "    def clear_gradients(self):\n",
    "        self.gradients.clear()\n",
    "\n",
    "    def reset_value(self, recursive=True):\n",
    "        \"\"\"\n",
    "        重置本节点的值，并递归重置本节点的下游节点的值\n",
    "        \"\"\"\n",
    "        self.value = None\n",
    "        if recursive:\n",
    "            for o in self.outputs:\n",
    "                o.reset_value()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}:{}'.format(self.__class__.__name__, self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlaceHolder(Node):\n",
    "\n",
    "    def __init__(self, shape=None, name='PlaceHolder'):\n",
    "        super().__init__([], name)\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable(Node):\n",
    "    def __init__(self, value=None, name='Variable'):\n",
    "        super().__init__([], name)\n",
    "        self.value = value\n",
    "\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] = grad_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Node):\n",
    "    def __init__(self, nodes, weights, bias, name='Linear'):\n",
    "        super().__init__(inputs=[nodes, weights, bias], name=name)\n",
    "        self.w_node = weights\n",
    "        self.x_node = nodes\n",
    "        self.b_node = bias\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"compute the wx + b using numpy\"\"\"\n",
    "        self.value = np.dot(self.x_node.value, self.w_node.value) + self.b_node.value\n",
    "\n",
    "    def backward(self):\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "        for node in self.outputs:\n",
    "            # gradient_of_loss_of_this_output_node = node.gradient[self]\n",
    "            grad_cost = node.gradients[self]\n",
    "\n",
    "            self.gradients[self.w_node] += np.dot(self.x_node.value.T, grad_cost)\n",
    "            self.gradients[self.b_node] += np.sum(grad_cost * 1, axis=0, keepdims=False)\n",
    "            self.gradients[self.x_node] += np.dot(grad_cost, self.w_node.value.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Node):\n",
    "    def __init__(self, node, name='Sigmoid'):\n",
    "        super().__init__(inputs=[node], name=name)\n",
    "        self.x_node = node\n",
    "        self.partial = None\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        z = np.where(-x > 1e2, 1e2, -x)\n",
    "        return 1.0 / (1 + np.exp(z))\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = self._sigmoid(self.x_node.value)\n",
    "\n",
    "    def backward(self):\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "        y = self.value\n",
    "        self.partial = y * (1 - y)\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self.x_node] += self.partial * grad_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Node):\n",
    "    \"\"\" 均方差loss \"\"\"\n",
    "    def __init__(self, y_true, y_hat, name='MSE'):\n",
    "        super().__init__(inputs=[y_true, y_hat], name=name)\n",
    "        self.y_true_node = y_true\n",
    "        self.y_hat_node = y_hat\n",
    "        self.diff = None\n",
    "\n",
    "    def forward(self):\n",
    "        y_true_flatten = self.y_true_node.value.reshape(-1, 1)\n",
    "        y_hat_flatten = self.y_hat_node.value.reshape(-1, 1)\n",
    "        self.diff = y_true_flatten - y_hat_flatten\n",
    "        self.value = np.mean(self.diff ** 2)\n",
    "\n",
    "    def backward(self):\n",
    "        n = self.y_hat_node.value.shape[0]\n",
    "        self.gradients[self.y_true_node] = (2 / n) * self.diff\n",
    "        self.gradients[self.y_hat_node] = (-2 / n) * self.diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(Node):\n",
    "    \"\"\"\n",
    "    优化器基类\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target, name='Optimizer'):\n",
    "        super().__init__(inputs=[target], name=name)\n",
    "        self.target = target\n",
    "\n",
    "    def forward(self):\n",
    "        # for node in self.graph.nodes:\n",
    "        #     node.forward()\n",
    "        self.backward()\n",
    "\n",
    "    def backward(self):\n",
    "        for node in self.graph.nodes[-2::-1]:\n",
    "            node.backward()\n",
    "        self.update()\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        抽象方法，利用梯度更新可训练变量\n",
    "        \"\"\"\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(Optimizer):\n",
    "    \"\"\"\n",
    "    梯度下降优化器\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target, learning_rate=0.01, name='GradientDescent'):\n",
    "        super().__init__(target, name=name)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def update(self):\n",
    "        for node in self.graph.params:\n",
    "            gradient = node.gradients[node]\n",
    "            node.value += -1 * self.learning_rate * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算图框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    \"\"\"\n",
    "    计算图类\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nodes = []  # 计算图内的节点的列表\n",
    "        self.params = []  # 变量节点\n",
    "\n",
    "    def add_node(self, node):\n",
    "        \"\"\"\n",
    "        添加节点\n",
    "        \"\"\"\n",
    "        self.nodes.append(node)\n",
    "        if isinstance(node, Variable):\n",
    "            self.params.append(node)\n",
    "\n",
    "    def clear_gradients(self):\n",
    "        \"\"\"\n",
    "        清除图中全部节点的雅可比矩阵\n",
    "        \"\"\"\n",
    "        for node in self.nodes:\n",
    "            node.clear_gradients()\n",
    "\n",
    "    def reset_value(self):\n",
    "        \"\"\"\n",
    "        重置图中全部节点的值\n",
    "        \"\"\"\n",
    "        for node in self.nodes:\n",
    "            node.reset_value(False)  # 每个节点不递归清除自己的子节点的值\n",
    "\n",
    "    def as_default(self):\n",
    "        global default_graph\n",
    "        default_graph = self\n",
    "\n",
    "\n",
    "# 全局默认计算图\n",
    "default_graph = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "class Session:\n",
    "\n",
    "    def __init__(self, graph=default_graph):\n",
    "        self.graph = graph\n",
    "        # self.topological_sort()\n",
    "\n",
    "    def topological_sort(self):\n",
    "        G = {}\n",
    "        nodes = [n for n in self.graph.nodes]\n",
    "        while len(nodes) > 0:\n",
    "            n = nodes.pop(0)\n",
    "            if n not in G:\n",
    "                G[n] = {'in': set(), 'out': set()}\n",
    "            for m in n.outputs:\n",
    "                if m not in G:\n",
    "                    G[m] = {'in': set(), 'out': set()}\n",
    "                G[n]['out'].add(m)\n",
    "                G[m]['in'].add(n)\n",
    "                nodes.append(m)\n",
    "\n",
    "        L = []\n",
    "        S = set(self.graph.nodes)\n",
    "        while len(S) > 0:\n",
    "            n = S.pop()\n",
    "            L.append(n)\n",
    "            for m in n.outputs:\n",
    "                G[n]['out'].remove(m)\n",
    "                G[m]['in'].remove(n)\n",
    "                # if no other incoming edges add to S\n",
    "                if len(G[m]['in']) == 0:\n",
    "                    S.add(m)\n",
    "\n",
    "        self.graph.nodes = L\n",
    "        return L\n",
    "\n",
    "    def eval(self, node):\n",
    "        for n in node.inputs:\n",
    "            self.eval(n)\n",
    "        node.forward()\n",
    "        return node.value\n",
    "\n",
    "    def run(self, fetches, feed_dict=None):\n",
    "        for k, v in feed_dict.items():\n",
    "            if isinstance(k, PlaceHolder):\n",
    "                k.value = v\n",
    "\n",
    "        fetches_result = []\n",
    "        for n in fetches:\n",
    "            if n.value is None:\n",
    "                fetches_result.append(self.eval(n))\n",
    "            else:\n",
    "                fetches_result.append(n.value)\n",
    "        return tuple(fetches_result)\n",
    "\n",
    "    @staticmethod\n",
    "    @contextmanager\n",
    "    def session(graph):\n",
    "        sess = Session(graph)\n",
    "        yield sess\n",
    "        graph.reset_value()\n",
    "        graph.clear_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal(shape, loc=0.0, scale=1.0):\n",
    "    return np.random.normal(loc=loc, scale=scale, size=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(data, axis=0):\n",
    "    mu = np.mean(data, axis=axis)\n",
    "    sigma = np.std(data, axis=axis, ddof=1)\n",
    "    return (data - mu) / sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建计算图\n",
    "graph = Graph()\n",
    "graph.as_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化参数\n",
    "n_features = dataset.shape[1]  # X特征数\n",
    "n_hidden = 100  # 隐藏层个数\n",
    "X = PlaceHolder(name='X')\n",
    "y = PlaceHolder(name='y')\n",
    "W1 = Variable(normal((n_features, n_hidden), scale=1), name='W1')\n",
    "b1 = Variable(np.zeros(n_hidden), name='b1')\n",
    "W2 = Variable(normal((n_hidden, 1), scale=1), name='W2')\n",
    "b2 = Variable(np.zeros(1), name='b2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "l1 = Linear(X, W1, b1, name='l1')\n",
    "h1 = Sigmoid(l1, name='h1')\n",
    "yhat = Linear(h1, W2, b2, name='yhat')\n",
    "loss = MSE(y, yhat, name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 5001\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建优化器\n",
    "optimizer = GradientDescent(loss, learning_rate=0.01, name='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss = 636.366\n",
      "Epoch: 101, loss = 17.328\n",
      "Epoch: 201, loss = 12.934\n",
      "Epoch: 301, loss = 9.982\n",
      "Epoch: 401, loss = 8.103\n",
      "Epoch: 501, loss = 7.313\n",
      "Epoch: 601, loss = 7.032\n",
      "Epoch: 701, loss = 5.730\n",
      "Epoch: 801, loss = 5.565\n",
      "Epoch: 901, loss = 5.010\n",
      "Epoch: 1001, loss = 4.735\n",
      "Epoch: 1101, loss = 4.281\n",
      "Epoch: 1201, loss = 4.285\n",
      "Epoch: 1301, loss = 4.286\n",
      "Epoch: 1401, loss = 4.151\n",
      "Epoch: 1501, loss = 3.873\n",
      "Epoch: 1601, loss = 3.766\n",
      "Epoch: 1701, loss = 3.711\n",
      "Epoch: 1801, loss = 3.451\n",
      "Epoch: 1901, loss = 3.352\n",
      "Epoch: 2001, loss = 3.177\n",
      "Epoch: 2101, loss = 3.201\n",
      "Epoch: 2201, loss = 3.380\n",
      "Epoch: 2301, loss = 3.027\n",
      "Epoch: 2401, loss = 2.778\n",
      "Epoch: 2501, loss = 2.741\n",
      "Epoch: 2601, loss = 2.699\n",
      "Epoch: 2701, loss = 2.797\n",
      "Epoch: 2801, loss = 3.246\n",
      "Epoch: 2901, loss = 2.610\n",
      "Epoch: 3001, loss = 2.379\n",
      "Epoch: 3101, loss = 2.596\n",
      "Epoch: 3201, loss = 2.840\n",
      "Epoch: 3301, loss = 2.364\n",
      "Epoch: 3401, loss = 2.381\n",
      "Epoch: 3501, loss = 2.974\n",
      "Epoch: 3601, loss = 2.266\n",
      "Epoch: 3701, loss = 2.367\n",
      "Epoch: 3801, loss = 1.864\n",
      "Epoch: 3901, loss = 2.292\n",
      "Epoch: 4001, loss = 2.374\n",
      "Epoch: 4101, loss = 2.077\n",
      "Epoch: 4201, loss = 2.107\n",
      "Epoch: 4301, loss = 2.229\n",
      "Epoch: 4401, loss = 1.933\n",
      "Epoch: 4501, loss = 1.723\n",
      "Epoch: 4601, loss = 1.903\n",
      "Epoch: 4701, loss = 2.082\n",
      "Epoch: 4801, loss = 1.651\n",
      "Epoch: 4901, loss = 2.382\n",
      "Epoch: 5001, loss = 1.723\n"
     ]
    }
   ],
   "source": [
    "with Session.session(graph) as sess:\n",
    "    for n in range(epoch):\n",
    "        data, label = shuffle(dataset, labels)\n",
    "        loss_sum = 0\n",
    "        n_step = len(dataset) // batch_size + 1\n",
    "        for i in range(n_step):\n",
    "            b = i * batch_size\n",
    "            e = b + batch_size\n",
    "            if e > len(dataset):\n",
    "                b = -batch_size\n",
    "                e = len(dataset)\n",
    "            batch_dataset = standardization(data[b:e])\n",
    "            batch_labels = label[b:e]\n",
    "            _, los = sess.run([optimizer, loss], feed_dict={X: batch_dataset, y: batch_labels})\n",
    "            # print('step: {}, loss = {:.3f}'.format(i+1, los))\n",
    "            loss_sum += los\n",
    "        if n % 100 == 0:\n",
    "            print('Epoch: {}, loss = {:.3f}'.format(n + 1, loss_sum / n_step))\n",
    "            losses.append(los)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAYi0lEQVR4nO3de4xc533e8e/vzJnb3shdcpeieBFlmY6kJLXiEooMpagiN66supULVIWdxhECAwxaubGBBIESoHAvMOAiieO4aF2osWGlSBwLtR0riZJKYRzYQRrblGPLkihKtKVQK1LcXd72NjvXX/84Z2ZnuUNxubvD5bzzfIDFzJw9M/OevTznmXcux9wdEREJS7TVAxARkc2ncBcRCZDCXUQkQAp3EZEAKdxFRAIUb/UAAHbu3OkHDhzY6mGIiPSUZ555Zsbdxzt977oI9wMHDnD06NGtHoaISE8xs7+/3Pc0LSMiEiCFu4hIgBTuIiIBUriLiARI4S4iEiCFu4hIgBTuIiIB6ulwP/7GHL/11HHOzpe3eigiIteVng73H07P89/+8gRTcwp3EZF2PR3u+Wwy/HKtscUjERG5vvR0uBfiDABL1foWj0RE5PrS0+Gu5i4i0llvh7uau4hIRz0d7gU1dxGRjno63NXcRUQ66+1wV3MXEemop8O9kE2ae1nNXURkhZ4O93ys5i4i0klPh3suE2GmOXcRkUv1dLibGfk4UnMXEblET4c7JPPuau4iIiv1fLjn44hyVc1dRKRdz4d7IZthqabmLiLSrufDXc1dRGS1ng93NXcRkdV6PtzV3EVEVuv5cFdzFxFZrefDXc1dRGS13g93NXcRkVV6P9zV3EVEVun5cC9kM5TV3EVEVuj5cFdzFxFZ7Yrhbmb7zOxrZnbMzJ43s4+ky8fM7Gkzezk9HU2Xm5l92sxOmNmzZvaObm6AXi0jIrLaWpp7Dfhld78NuAt42MxuBx4Bjrj7QeBIehngPcDB9Osw8JlNH3WbfBxRrTv1hnfzbkREesoVw93dT7v7d9Lzc8AxYA/wAPBYutpjwPvS8w8Av+eJvwW2m9nuTR95qnU0JrV3EZGWq5pzN7MDwE8A3wR2uftpSHYAwES62h7gtbarTabLuqJ1NCbNu4uItKw53M1sCPgS8FF3n32zVTssWzVnYmaHzeyomR2dnp5e6zBWaTZ3zbuLiCxbU7ibWZYk2H/f3b+cLj7TnG5JT6fS5ZPAvrar7wVOXXqb7v6oux9y90Pj4+PrHb+au4hIB2t5tYwBnwWOufsn2771BPBQev4h4Ktty38+fdXMXcDF5vRNN6i5i4isFq9hnbuBDwLfN7Pvpst+HfgE8LiZfQg4CTyYfu9J4H7gBLAI/MKmjvgSau4iIqtdMdzd/a/pPI8O8K4O6zvw8AbHtWat5q7jqIqItATxDlWAck3NXUSkqefDXc1dRGS1ng93NXcRkdV6PtzV3EVEVuv5cFdzFxFZrffDXc1dRGSV3g93NXcRkVXCCXc1dxGRlp4PdzNLjsak5i4i0tLz4Q7p0ZjU3EVEWoIIdzV3EZGVggh3NXcRkZWCCHc1dxGRlYIIdzV3EZGVggh3NXcRkZWCCHc1dxGRlYIIdzV3EZGVggh3NXcRkZWCCHc1dxGRlcII92yGJR0gW0SkJYxwjyPKNU3LiIg0BRHuhWyGspq7iEhLEOGejyMq9QaNhm/1UERErgtBhHvzOKp6UlVEJBFEuC8fjUnz7iIiEEi4F1rHUVVzFxGBQMJdzV1EZKUgwl3NXURkpSDCXc1dRGSlIMJdzV1EZKUgwj2fVXMXEWkXRLgXYjV3EZF2QYS7mruIyEpBhLuau4jISlcMdzP7nJlNmdlzbcv+o5m9bmbfTb/ub/ver5nZCTM7bmb/tFsDb6fmLiKy0lqa++eB+zos/213vyP9ehLAzG4H3g/8aHqd/2Fmmc0a7OWouYuIrHTFcHf3rwPn1nh7DwB/6O5ld38FOAHcuYHxrYmau4jIShuZc/+wmT2bTtuMpsv2AK+1rTOZLlvFzA6b2VEzOzo9Pb2BYSy/iUnNXUQksd5w/wxwC3AHcBr4rXS5dVi344esu/uj7n7I3Q+Nj4+vcxjpnZqR09GYRERa1hXu7n7G3evu3gD+F8tTL5PAvrZV9wKnNjbEtSnEkY7GJCKSWle4m9nutov/Emi+kuYJ4P1mljezm4GDwLc2NsS1yWczau4iIqn4SiuY2ReAe4CdZjYJfAy4x8zuIJlyeRX4RQB3f97MHgdeAGrAw+5+TRK3kI005y4ikrpiuLv7Bzos/uybrP9x4OMbGdR65GM1dxGRpiDeoQpq7iIi7YIJdzV3EZFlwYS7mruIyLJgwl3NXURkWTDhruYuIrIsmHBXcxcRWRZMuKu5i4gsCybc83GGclXNXUQEQgr3bMRSTc1dRAQCCvdCnKFSa+De8UMoRUT6SjDhvnzADrV3EZFgwr15qD197K+ISEDh3mzuS3o5pIhIOOGu5i4isiyYcFdzFxFZFky4q7mLiCwLJtzV3EVElgUT7oWsmruISFMw4Z6P0+aujyAQEQkn3FvNXW9iEhEJJ9zV3EVElgUT7mruIiLLggl3NXcRkWXBhLuau4jIsmDCPZdRcxcRaQom3KPIyMWRmruICAGFOyTz7mruIiKBhXshm1FzFxEhsHDPx5EOki0iQmDhruYuIpIIKtw15y4ikggq3NXcRUQSQYW7mruISCKocFdzFxFJXDHczexzZjZlZs+1LRszs6fN7OX0dDRdbmb2aTM7YWbPmtk7ujn4S6m5i4gk1tLcPw/cd8myR4Aj7n4QOJJeBngPcDD9Ogx8ZnOGuTZq7iIiiSuGu7t/HTh3yeIHgMfS848B72tb/nue+Ftgu5nt3qzBXomau4hIYr1z7rvc/TRAejqRLt8DvNa23mS6bBUzO2xmR83s6PT09DqHsZKau4hIYrOfULUOy7zTiu7+qLsfcvdD4+Pjm3Lnau4iIon1hvuZ5nRLejqVLp8E9rWttxc4tf7hXZ182tzdO+5PRET6xnrD/QngofT8Q8BX25b/fPqqmbuAi83pm2uheTQmTc2ISL+Lr7SCmX0BuAfYaWaTwMeATwCPm9mHgJPAg+nqTwL3AyeAReAXujDmy2o/GlPzvIhIP7piuLv7By7zrXd1WNeBhzc6qPVqNfdqHYrZrRqGiMiWC+4dqqBpGRGRoMK92dz1ihkR6XdBhbuau4hIIqhwV3MXEUkEFe5q7iIiiaDCXc1dRCQRVLiruYuIJIIKdzV3EZFEUOGu5i4ikggq3NXcRUQSQYW7mruISCKocFdzFxFJBBXuUWTkMpGau4j0vaDCHXQ0JhERCDHcdRxVEZEAw13NXUQkvHAvZDXnLiISXLjn40xyJCYRkT4WXLiruYuIBBju+TijOXcR6XvBhbuau4hIgOGu5i4iEmC4q7mLiAQY7mruIiIBhruau4hIgOGez6q5i4gEF+6FOGnu7r7VQxER2TLBhXs+m8EdKnVNzYhI/wov3NMDdmjeXUT6WXjhnh5qT/PuItLPggv3QrO5V9XcRaR/BRfu+dZBstXcRaR/BRfuhdZBstXcRaR/BRfuau4iIhBv5Mpm9iowB9SBmrsfMrMx4IvAAeBV4F+7+/mNDXPtNOcuIrI5zf2n3f0Odz+UXn4EOOLuB4Ej6eVrpvVqGTV3Eelj3ZiWeQB4LD3/GPC+LtzHZRWyau4iIhsNdweeMrNnzOxwumyXu58GSE8nOl3RzA6b2VEzOzo9Pb3BYSzLx2ruIiIbmnMH7nb3U2Y2ATxtZi+u9Yru/ijwKMChQ4c27YNg1NxFRDbY3N39VHo6BXwFuBM4Y2a7AdLTqY0O8mq0mrveoSoifWzd4W5mg2Y23DwPvBt4DngCeChd7SHgqxsd5NVoNXd9toyI9LGNTMvsAr5iZs3b+QN3/3Mz+zbwuJl9CDgJPLjxYa7dcnNXuItI/1p3uLv7D4G3d1h+FnjXRga1EZnIyGZMb2ISkb4W3DtUoXkcVTV3EelfQYZ7chxVNXcR6V9Bhruau4j0uzDDXc1dRPpcmOGu5i4ifS7IcNecu4j0uyDDPR9H+vgBEelrQYZ7IZtRcxeRvhZkuOfjSHPuItLXggx3NXcR6XdBhruau4j0uyDDXc1dRPpdkOGu5i4i/S7IcG82d/dNO8CTiEhPCTLc83FEw6FaV7iLSH8KMtwL2eSAHZp3F5F+FWS45+NkszTvLiL9KsxwV3MXkT4XZriruYtInwsy3DXnLiL9LuhwV3MXkX4VZLjvGMwB8DtHXmZqbmmLRyMicu0FGe4/tmcb/+WBH+WbPzzLfZ/6Bk89/8ZWD0lE5JoKMtwBPvjOA/zpL/0UN24vcPh/P8MjX3qWhXJtq4clInJNBBvuAG+dGObL//Zu/t09t/DFo69x/6e/wXdOnt/qYYmIdF3Q4Q6QiyN+9b5b+eLhd1KrOw/+z//Hf/ij5zQXLyJBCz7cm+68eYw/++g/4mfv3M8XvnWSe37jr/jkU8eZW6pu9dBERDadXQ+fnHjo0CE/evToNbu/V2YW+M2njvOnz55mbDDHv7/3rfzsT+4nH2eu2RhERDbKzJ5x90Mdv9eP4d707OQFPvFnL/I3PzjL3tEiH7zrJt779hvZs714zcciInK1FO5vwt35xsszfOovXuI7Jy8k47lplH/+9hu5/8d3Mz6cv+L1p+fLTJ4v8dq5RSbPlxgfynPvbRPsHHrz64qIbITCfY1Onl3kj589xR9/7xQvvjFHZPDOW3Zw47Yi1XqDSr1BpebJ+VqDqbklJs+XKNdWvxPWDN6xf5SfuX0X7759F28ZH1q1Tr3hXCwlc/6jA1nMrOvbKCLhULivw0tn5viT753iz59/g/mlGtk4IpuJyGUisnFELmPsGMyzb6zI3tGB1une0SKvzCzw9AtnePqFMzx/ahaAW8YHuWV8iPOLFc4tJF8XSlWaP/7BXIZ9YwPsHxvgph3N00F+5IZhJobzCn4RWUXhvoVev1DiL9Kgn5pbYmwwx47BPKODWcYG84wNZGk4nDy3yGvnFjmZfrU/Gtg+kOVtu4a59YZh3rZrmLdODDExnGd8OM9QPl4V/LNLVV4+M8/LZ+Z4eWqe18+XGB3MMj5cYNdInom20+FCzEAus6GdR63eIBPZhm6jWm9wdr5CnDFNZ4mskcK9xzQazsx8mRPT87z0xhzHz8xz/I1ZXjozz/wl77LNxxHjw3l2DuUpZjO8MrPAG7PLr+EvZCP2bC9ysVTj7EKZTr/uyGAwFzNUiBnKxwzkYyIDd3AAdxoOjlOpNShV65QqDcrVOqVqnVrDW7cxmI8ZzGcYyifnC9kMcWRkMxFxxoijiGzGKFXrTM+VmZkvMz1X5vzi8ktSdw7luW33MLfvHuG23SPcfuMI+0YH6LTvqNQbzMyVmZmvtG5rZr7M3FKN4ULMtmKWkUKWkWLMSCHLUCEmMiMyw4z0PGSi5JHYSHH1znIjqvUGP5ie5/gbc0Rm7B0tsme0yPjQ5j0aq9UbnFusUG84u4YLRNHV3e5S+rs4M7vEmdkyF0oVdm8rcGDHIPvGBshm+uYV0z1nS8LdzO4DfgfIAL/r7p+43LoK97Vxd16/UOKVmYUVwTgzX2F6rsxCpcbNOwc5ODHM23YNcXBimD2jRTLpP3ut3mBmvsKZ2SWm5spMzS0xv1Rjvpx+tZ1vsjT8LD2fjyMK2QyFbIZiNkMxF5GPM9TqDebLdebLVRbK9dbtlGt1avXkeYpaw1vn89mI8aF8a8fUPF2q1jl2eo5jp2c5MTVPpX71n+zZ3NHMV2odd2ZvJhcn45oYyTM+lGfncB4jCela3ak2nGot2ZaBXIZtxeyKr6FCzKkLJV44PcuLp+cuuw35ONnp7hktMlyIcYdGcyeanibbYmSiZOfT3ClVag3OLVQ4u1Dm7EKFC207xlwmYu9YkQM7BltTfDuG8lxcrHBuocq59DrnFiqcna9wZm5pxfUvlYmSHdJNOwbZP1YkMqOa/g6bP5Nao8FIIdv6He4cTn52O4ZyXCxVef18idcvpF/nS5y6UCLOROwcyrFjMMfYYLLuzqHkfPLoNsfYUI7hDo9M27k78+UaF0tVLixWmS1VuViqUqrWyWbSqdTYWufzccRI+rsaKWTJxdGq25tdqnFxscqFUoX5pRqZyJJp2SgiGycFpfl/UMxlGMhmVuxQa/UGpy8utR6Fv3ZukdcvlBgpZNk3VmTf6EBrKndbcWPPtV3zcDezDPAS8DPAJPBt4APu/kKn9RXu0kmz9R47Pcvpi53fUZyNInYO55JQSb/GBnNkIqPRcBYqyT/+bKnG7FKV+aXaihB1klCt1b21s5yeK7d2fmfnK5hBHCWPPLKZiDgyMlHy6CO57WorjJsmhvPcunuk9QjkR24YBmDyXBJyk+cX09MSi5U6UfooorUzTf/f643kkVzdPRl3w8lmIsYGc61t3ZGGZBRZEihnF3n17CInzy6wUFl5TINtxSxjg7lWgO4aSafoRgqt89uKWU5dKPHKzCKvzizwytkFXp1Z4PULpeRnnj731Px5ZMy4WKoyM1+mdukPos3oQJY9o0Vu3Fak4c7MfHMnU141ztbvN2OMDuQoZDPUG8mOJNmhOPWGU6rWqb/JfV5JMZthpBhTzGaSUC9V13V7+ThiIJchF0fMzFdW3EYcGTdsKzBbqjK7tPKR93A+5hf/8Vv48L0H1zX+Nwv3eF23eGV3Aifc/YfpAP4QeADoGO4inWQzEbfeMMKtN4ys6/pRZAwXsgwXsjC6yYNr094eZ0s1Jkbyl33eYL3bst5xNVv69oEsowO5NU+x7N5W5B/eNHZV99dIX/3VekS5UGGkELN3tMiN24sM5C4fN0vVOjPz5fQRSYVzzeBfqHBuoUyl1iBOd6zN6b04MgrZtkdPA8uPoIrZDLXG8qvbmq92W6rWWzv6i4vV5LRUZanaYKQYs72YY3t6O9sHcgzlYxruy4/c6g2qjfbpyRoL5WR6crFSo1xtMDGSZ//YQOsFEru3LT96vliqtl4yPXk+OX3rxPBV/ZzXqlvhvgd4re3yJPCT7SuY2WHgMMD+/fu7NAyR7jO7NjuRq2VmrUcz10IUGaODOUYHcxzcdXWBVchm0lebDXRpdNeHbcUs2/Zs48f2bOv6fXXrmZJOk0grHuu4+6PufsjdD42Pj3dpGCIi/alb4T4J7Gu7vBc41aX7EhGRS3Qr3L8NHDSzm80sB7wfeKJL9yUiIpfoypy7u9fM7MPA/yV5KeTn3P35btyXiIis1q0nVHH3J4Enu3X7IiJyeXrrmYhIgBTuIiIBUriLiATouvjgMDObBv5+nVffCcxs4nB6gba5P2ib+8NGtvkmd+/4RqHrItw3wsyOXu6zFUKlbe4P2ub+0K1t1rSMiEiAFO4iIgEKIdwf3eoBbAFtc3/QNveHrmxzz8+5i4jIaiE0dxERuYTCXUQkQD0d7mZ2n5kdN7MTZvbIVo+nG8zsc2Y2ZWbPtS0bM7Onzezl9PQ6OkTExpnZPjP7mpkdM7Pnzewj6fJgt9vMCmb2LTP7XrrN/yldfrOZfTPd5i+mn7IaDDPLmNnfmdmfpJdD395Xzez7ZvZdMzuaLuvK33XPhnt6nNb/DrwHuB34gJndvrWj6orPA/ddsuwR4Ii7HwSOpJdDUgN+2d1vA+4CHk5/tyFvdxm4193fDtwB3GdmdwH/FfjtdJvPAx/awjF2w0eAY22XQ99egJ929zvaXtvelb/rng132o7T6u4VoHmc1qC4+9eBc5csfgB4LD3/GPC+azqoLnP30+7+nfT8HMk//x4C3m5PzKcXs+mXA/cC/yddHtQ2m9le4J8Bv5teNgLe3jfRlb/rXg73Tsdp3bNFY7nWdrn7aUiCEJjY4vF0jZkdAH4C+CaBb3c6RfFdYAp4GvgBcMHda+kqof2Nfwr4VaCRXt5B2NsLyQ77KTN7Jj2ONHTp77prn+d+DVzxOK3S28xsCPgS8FF3n02KXbjcvQ7cYWbbga8At3Va7dqOqjvM7L3AlLs/Y2b3NBd3WDWI7W1zt7ufMrMJ4Gkze7Fbd9TLzb2fj9N6xsx2A6SnU1s8nk1nZlmSYP99d/9yujj47QZw9wvAX5E837DdzJolLKS/8buBf2Fmr5JMqd5L0uRD3V4A3P1UejpFsgO/ky79XfdyuPfzcVqfAB5Kzz8EfHULx7Lp0rnXzwLH3P2Tbd8KdrvNbDxt7JhZEfgnJM81fA34V+lqwWyzu/+au+919wMk/7t/6e7/hkC3F8DMBs1suHkeeDfwHF36u+7pd6ia2f0ke/vmcVo/vsVD2nRm9gXgHpKPBT0DfAz4I+BxYD9wEnjQ3S990rVnmdlPAd8Avs/yfOyvk8y7B7ndZvYPSJ5My5CUrsfd/T+b2VtImu0Y8HfAz7l7eetGuvnSaZlfcff3hry96bZ9Jb0YA3/g7h83sx104e+6p8NdREQ66+VpGRERuQyFu4hIgBTuIiIBUriLiARI4S4iEiCFu4hIgBTuIiIB+v9mrNSStKcYQAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 豆瓣评论分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_file = r'D:\\development\\jupyter\\datasource\\movie_comments.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\development\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "comment = pd.read_csv(comment_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2  3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3  4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4  5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = []\n",
    "stars = []\n",
    "for i in range(len(comment['star'])):\n",
    "    try:\n",
    "        stars.append(int(comment['star'][i]))\n",
    "        comments.append(comment['comment'][i])\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star = set()\n",
    "for i in stars:\n",
    "    star.add(i)\n",
    "star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    2\n",
       "3    4\n",
       "4    1\n",
       "5    1\n",
       "6    2\n",
       "7    4\n",
       "8    4\n",
       "9    1\n",
       "Name: star, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment['star'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stop_words(path):\n",
    "    stop_words = set()\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            stop_words.add(line.strip())\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = load_stop_words(r'D:\\development\\jupyter\\corpus\\stop_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(string):\n",
    "    return ' '.join(re.findall(r'[\\d|\\w]+', string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(text):\n",
    "    return [w for w in jieba.lcut(token(text)) if len(w.strip()) > 0 and w not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮番上场，视物理逻辑于不顾，不得不说有钱真好，随意胡闹'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = comment['comment'][1]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\noone\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.608 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['首映礼',\n",
       " '太',\n",
       " '恐怖',\n",
       " '电影',\n",
       " '不讲道理',\n",
       " '吴京',\n",
       " '粉红',\n",
       " '英雄',\n",
       " '梦',\n",
       " '装备',\n",
       " '轮番',\n",
       " '上场',\n",
       " '视',\n",
       " '物理',\n",
       " '逻辑',\n",
       " '不顾',\n",
       " '说',\n",
       " '有钱',\n",
       " '真',\n",
       " '随意',\n",
       " '胡闹']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = cut(token(t))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec.load(r'D:\\development\\jupyter\\corpus\\zhwiki_sougou_news_word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vec(sentence):\n",
    "    return np.mean([word2vec.wv[w] for w in cut(sentence) if w in word2vec.wv], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.5, 3.5, 4.5])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([[1,2,3],[4,5,6]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.4314901 , -0.21323907,  1.8681071 ,  0.08058092,  0.5520421 ,\n",
       "        0.94416606,  0.16812155, -0.5000285 , -0.8309715 , -0.9423348 ,\n",
       "        0.12564968, -0.662259  ,  0.681413  , -0.4508646 ,  0.6692805 ,\n",
       "       -0.9565575 , -0.3335797 , -1.327392  ,  0.9559461 , -0.9573267 ,\n",
       "        1.3100893 ,  0.4411884 ,  0.11875989,  0.02037793,  0.6158737 ,\n",
       "       -0.73443556, -0.8963304 ,  1.462498  ,  0.01595178,  0.45038795,\n",
       "        1.0694367 , -0.6588845 , -0.56813586, -0.40205342,  0.27342457,\n",
       "       -0.36333033, -0.3247346 ,  0.7777479 ,  0.10208637,  0.83331925,\n",
       "        0.16834512, -0.18541825, -0.530091  ,  0.14174439, -0.441872  ,\n",
       "        0.41390347,  0.21868041,  0.8984635 ,  0.50942695, -0.42627645,\n",
       "       -1.1100296 , -0.11748734, -1.0356447 , -1.22351   , -0.00699306,\n",
       "        1.7741734 , -0.64081633,  0.40560696,  1.4500227 , -0.01240737,\n",
       "        1.2645762 , -0.06243144, -0.360958  , -0.37007433,  0.3438284 ,\n",
       "        0.13069817, -0.26574332, -0.17688648, -0.22753856, -1.267106  ,\n",
       "        0.53771126, -0.6961406 ,  0.1631657 ,  1.0532548 , -0.15809627,\n",
       "        1.3245733 ,  1.3743098 , -0.4477293 ,  0.8957669 ,  0.1840019 ,\n",
       "       -0.83338803,  0.07800856,  0.3575894 ,  0.01160158, -0.7405318 ,\n",
       "        0.6367809 ,  1.266562  ,  0.43800187,  1.0269845 , -0.6336646 ,\n",
       "        0.5328878 , -0.459705  , -0.16497858,  0.26396057, -1.1179649 ,\n",
       "       -0.05335836, -0.54722035,  0.2610099 ,  0.5927758 , -0.22406682],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2vec(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = [sentence2vec(i) for i in comments[:10000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-139-61894c065cc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "np.isnan(dataset2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义句子向量模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用普林斯顿的句子向量论文A SIMPLE BUT TOUGH-TO-BEAT BASELINE FOR SENTENCE EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "def get_weighted_average(We, x, w):\n",
    "    \"\"\"\n",
    "    Compute the weighted average vectors\n",
    "    :param We: We[i,:] is the vector for word i\n",
    "    :param x: x[i, :] are the indices of the words in sentence i\n",
    "    :param w: w[i, :] are the weights for the words in sentence i\n",
    "    :return: emb[i, :] are the weighted average vector for sentence i\n",
    "    \"\"\"\n",
    "    n_samples = x.shape[0]\n",
    "    emb = np.zeros((n_samples, We.shape[1]))\n",
    "    for i in range(n_samples):\n",
    "        emb[i, :] = w[i, :].dot(We[x[i, :], :]) / np.count_nonzero(w[i, :])\n",
    "    return emb\n",
    "\n",
    "\n",
    "def compute_pc(X, npc=1):\n",
    "    \"\"\"\n",
    "    Compute the principal components. DO NOT MAKE THE DATA ZERO MEAN!\n",
    "    :param X: X[i,:] is a data point\n",
    "    :param npc: number of principal components to remove\n",
    "    :return: component_[i,:] is the i-th pc\n",
    "    \"\"\"\n",
    "    svd = TruncatedSVD(n_components=npc, n_iter=7, random_state=0)\n",
    "    svd.fit(X)\n",
    "    return svd.components_\n",
    "\n",
    "\n",
    "def remove_pc(X, npc=1):\n",
    "    \"\"\"\n",
    "    Remove the projection on the principal components\n",
    "    :param X: X[i,:] is a data point\n",
    "    :param npc: number of principal components to remove\n",
    "    :return: XX[i, :] is the data point after removing its projection\n",
    "    \"\"\"\n",
    "    pc = compute_pc(X, npc)\n",
    "    if npc == 1:\n",
    "        XX = X - X.dot(pc.transpose()) * pc\n",
    "    else:\n",
    "        XX = X - X.dot(pc.transpose()).dot(pc)\n",
    "    return XX\n",
    "\n",
    "\n",
    "def SIF_embedding(We, x, w, params):\n",
    "    \"\"\"\n",
    "    Compute the scores between pairs of sentences using weighted average + removing the projection on the first principal component\n",
    "    :param We: We[i,:] is the vector for word i\n",
    "    :param x: x[i, :] are the indices of the words in the i-th sentence\n",
    "    :param w: w[i, :] are the weights for the words in the i-th sentence\n",
    "    :param params.rmpc: if >0, remove the projections of the sentence embeddings to their first principal component\n",
    "    :return: emb, emb[i, :] is the embedding for sentence i\n",
    "    \"\"\"\n",
    "    emb = get_weighted_average(We, x, w)\n",
    "    if params.rmpc > 0:\n",
    "        emb = remove_pc(emb, params.rmpc)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def getWordWeight(word_frequency, a=1e-3):\n",
    "    if a <= 0:  # when the parameter makes no sense, use unweighted\n",
    "        a = 1.0\n",
    "    word2weight = {}\n",
    "    N = 0\n",
    "    for w, f in word_frequency.items():\n",
    "        word2weight[w] = f\n",
    "        N += f\n",
    "    for key, value in word2weight.items():\n",
    "        word2weight[key] = a / (a + value / N)\n",
    "    return word2weight\n",
    "\n",
    "\n",
    "def sentences2idx(sentences, words):\n",
    "    \"\"\"\n",
    "    Given a list of sentences, output array of word indices that can be fed into the algorithms.\n",
    "    :param sentences: a list of sentences\n",
    "    :param words: a dictionary, words['str'] is the indices of the word 'str'\n",
    "    :return: x1, m1. x1[i, :] is the word indices in sentence i, m1[i,:] is the mask for sentence i (0 means no word at the location)\n",
    "    \"\"\"\n",
    "    seq1 = []\n",
    "    for s in sentences:\n",
    "        seq1.append(getSeq(s, words))\n",
    "    x1, m1 = prepare_data(seq1)\n",
    "    return x1, m1\n",
    "\n",
    "\n",
    "def prepare_data(list_of_seqs):\n",
    "    lengths = [len(s) for s in list_of_seqs]\n",
    "    n_samples = len(list_of_seqs)\n",
    "    maxlen = np.max(lengths)\n",
    "    x = np.zeros((n_samples, maxlen)).astype('int32')\n",
    "    x_mask = np.zeros((n_samples, maxlen)).astype('float32')\n",
    "    for idx, s in enumerate(list_of_seqs):\n",
    "        x[idx, :lengths[idx]] = s\n",
    "        x_mask[idx, :lengths[idx]] = 1.\n",
    "    x_mask = np.asarray(x_mask, dtype='float32')\n",
    "    return x, x_mask\n",
    "\n",
    "\n",
    "def getSeq(s, words_map):\n",
    "    idx = []\n",
    "    for w in s:\n",
    "        if w in words_map:\n",
    "            idx.append(words_map[w])\n",
    "        else:\n",
    "            idx.append(len(words_map) - 1)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def getWeight(words, word2weight):\n",
    "    weight4ind = {}\n",
    "    for word, ind in words.items():\n",
    "        if word in word2weight:\n",
    "            weight4ind[ind] = word2weight[word]\n",
    "        else:\n",
    "            weight4ind[ind] = 1.0\n",
    "    return weight4ind\n",
    "\n",
    "\n",
    "def seq2weight(seq, mask, weight4ind):\n",
    "    weight = np.zeros(seq.shape).astype('float32')\n",
    "    for i in range(seq.shape[0]):\n",
    "        for j in range(seq.shape[1]):\n",
    "            if mask[i, j] > 0 and seq[i, j] >= 0:\n",
    "                weight[i, j] = weight4ind[seq[i, j]]\n",
    "    weight = np.asarray(weight, dtype='float32')\n",
    "    return weight\n",
    "\n",
    "\n",
    "class params(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.LW = 1e-5\n",
    "        self.LC = 1e-5\n",
    "        self.eta = 0.05\n",
    "\n",
    "    def __str__(self):\n",
    "        t = \"LW\", self.LW, \", LC\", self.LC, \", eta\", self.eta\n",
    "        t = map(str, t)\n",
    "        return ' '.join(t)\n",
    "\n",
    "\n",
    "class SIFModel:\n",
    "    def __init__(self, word2vec_model, weightpara=1e-3):\n",
    "        \"\"\"\n",
    "        :param word2vec_model: word2vec模型\n",
    "        :param stop_words: 停用词\n",
    "        :param weightpara:\n",
    "        \"\"\"\n",
    "        self.word2vec_model = word2vec_model\n",
    "        words = word2vec_model.wv.index2word\n",
    "        self.word_vectors = word2vec_model.wv.vectors\n",
    "        word_frequency = {w: v.count for w, v in word2vec_model.wv.vocab.items()}\n",
    "        self.word_index_map = {w: n for n, w in enumerate(words)}\n",
    "        word2weight = getWordWeight(word_frequency, weightpara)\n",
    "        self.weight4ind = getWeight(self.word_index_map, word2weight)\n",
    "\n",
    "    def sentence2vec(self, sentences):\n",
    "        \"\"\"\n",
    "        计算句子向量\n",
    "        :param sentences:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sens = [cut(s) for s in sentences]\n",
    "        x, m = sentences2idx(sens, self.word_index_map)\n",
    "        w = seq2weight(x, m, self.weight4ind)\n",
    "        p = params()\n",
    "        p.rmpc = 0 if len(sentences) <= 1 else 1\n",
    "        return SIF_embedding(self.word_vectors, x, w, p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建模型\n",
    "sif_model = SIFModel(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 100),\n",
       " array([[-0.37148577, -0.14582501,  1.61984146,  0.07227427,  0.52611715,\n",
       "          0.79112673,  0.15752387, -0.40599465, -0.79713565, -0.8083415 ,\n",
       "          0.08923913, -0.62931818,  0.60452098, -0.24124436,  0.66816014,\n",
       "         -1.01637352, -0.36557609, -1.32418454,  0.83325166, -0.73187137,\n",
       "          1.06585884,  0.40391096,  0.11407755,  0.08116055,  0.63171309,\n",
       "         -0.565512  , -0.84652507,  1.29514861,  0.01530122,  0.47093913,\n",
       "          0.95852309, -0.57534248, -0.4818776 , -0.30580068,  0.1150021 ,\n",
       "         -0.34451789, -0.20868465,  0.63513392,  0.02345112,  0.86482495,\n",
       "          0.09472033, -0.19055696, -0.47505215,  0.23752721, -0.37120318,\n",
       "          0.41446254,  0.18967609,  0.68884474,  0.40239415, -0.30909076,\n",
       "         -0.88811129, -0.13006161, -0.81700778, -1.11770785,  0.09335148,\n",
       "          1.65184915, -0.47939846,  0.43245053,  1.2934233 , -0.025197  ,\n",
       "          1.20240521, -0.05018903, -0.31783891, -0.3727361 ,  0.3252449 ,\n",
       "          0.05112781, -0.25066045, -0.18854585, -0.21965767, -1.19547057,\n",
       "          0.41887119, -0.56918257,  0.10705295,  0.87859082, -0.07804767,\n",
       "          1.16508389,  1.18367851, -0.40988749,  0.82371366,  0.08879372,\n",
       "         -0.73152906,  0.08550815,  0.41372603,  0.03473616, -0.68736064,\n",
       "          0.6375066 ,  1.0901767 ,  0.3864713 ,  0.9366129 , -0.48942074,\n",
       "          0.54992163, -0.391918  , -0.08168498,  0.22358182, -0.97959018,\n",
       "          0.04849971, -0.48889306,  0.22752939,  0.43288526, -0.12990345]]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2v = sif_model.sentence2vec([t])\n",
    "s2v.shape, s2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2v[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\development\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "dataset = np.array([sif_model.sentence2vec([i])[0] for i in comments[:10000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0:100, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08239317, -0.42906678,  0.79234445,  0.02816611,  0.62318861,\n",
       "        1.21528304, -0.50583208, -0.05945163, -0.83206856,  0.2146053 ,\n",
       "        1.5383215 ,  0.09208171,  1.47147059,  0.08726862,  0.38526055,\n",
       "       -0.07311188, -1.01709723,  0.31261376,  1.20884979, -1.48534298,\n",
       "        2.40975857,  0.74284488, -0.39396742,  0.07520241, -0.70188618,\n",
       "       -0.54689884,  0.3483099 ,  0.65707636,  0.44856936, -0.05704463,\n",
       "        1.43716204, -0.72094291, -0.81099075,  0.37981534, -0.18492876,\n",
       "        0.12555869, -0.47824076,  0.37308496,  0.52149045, -0.38853732,\n",
       "       -0.29278687,  0.29526719, -1.55026436, -0.8495968 ,  0.1184432 ,\n",
       "        0.38903669,  0.51281732,  1.38472247,  0.40694526, -1.6350652 ,\n",
       "       -0.0486272 , -0.08006042, -0.55214089, -0.58384514, -0.38205394,\n",
       "        1.39344573, -0.23462686, -0.48398241,  1.03827929,  0.75543386,\n",
       "        1.68546772,  0.59013867,  0.11416898,  0.09117563, -1.57665789,\n",
       "       -1.05298233,  0.16447324, -1.41008735, -0.45793965, -1.05336249,\n",
       "        0.76167798, -0.94914049,  0.17357141, -0.95546716,  0.10129005,\n",
       "       -0.15027091,  0.60723191,  0.83899009, -0.37988514, -0.25839743,\n",
       "       -1.06469023,  1.81042993, -0.44241196, -0.39523059, -0.5990935 ,\n",
       "       -0.71896911,  0.76976281,  0.96182662,  0.61456603, -0.4069213 ,\n",
       "       -0.22070466,  0.3391147 , -0.07172307,  0.35913387, -1.67620182,\n",
       "       -0.19047202, -0.74263674,  0.66940904,  0.05345455, -0.66406745])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(stars[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (np.arange(5) == labels[:,None]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 5)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0:100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8900"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(dataset).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(df.fillna(0).values).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.fillna(0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feature = 100\n",
    "num_labels = 5\n",
    "num_nodes= 1024\n",
    "batch_size = 100\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, n_feature))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "\n",
    "    # Variables.\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([n_feature, num_nodes]))\n",
    "    biases_1 = tf.Variable(tf.zeros([num_nodes]))\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([num_nodes, num_labels]))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1) # add relu layer\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    softmax = tf.nn.softmax(logits_2)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits_2, labels = tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "\n",
    "    # Predictions for the training\n",
    "    train_prediction = tf.nn.softmax(logits_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (labels.shape[0] - batch_size)\n",
    "        batch_data = dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 100 == 0):\n",
    "            print(\"Minibatch loss at step {}: loss={}, accuracy={}\".format(step, l, accuracy(predictions, batch_labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
