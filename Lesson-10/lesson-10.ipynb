{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从零开始实现神经网络框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data['data']\n",
    "labels = data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    计算图节点类基类\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputs=None, name=''):\n",
    "        if inputs is None:\n",
    "            inputs = []\n",
    "        self.inputs = inputs\n",
    "        self.name = name\n",
    "        self.value = None\n",
    "        self.outputs = []\n",
    "        self.gradients = {}  # 梯度\n",
    "        self.graph = default_graph  # 计算图对象，默认为全局对象default_graph\n",
    "        self.params = []\n",
    "\n",
    "        for node in self.inputs:\n",
    "            node.outputs.append(self)  # 建立节点之间的连接\n",
    "\n",
    "        # 将本节点添加到计算图中\n",
    "        self.graph.add_node(self)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        前向传播计算本节点的值\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        反向传播，计算结果节点对本节点的梯度\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "    def clear_gradients(self):\n",
    "        self.gradients.clear()\n",
    "\n",
    "    def reset_value(self, recursive=True):\n",
    "        \"\"\"\n",
    "        重置本节点的值，并递归重置本节点的下游节点的值\n",
    "        \"\"\"\n",
    "        self.value = None\n",
    "        if recursive:\n",
    "            for o in self.outputs:\n",
    "                o.reset_value()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}:{}'.format(self.__class__.__name__, self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlaceHolder(Node):\n",
    "\n",
    "    def __init__(self, shape=None, name='PlaceHolder'):\n",
    "        super().__init__([], name)\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable(Node):\n",
    "    def __init__(self, value=None, name='Variable'):\n",
    "        super().__init__([], name)\n",
    "        self.value = value\n",
    "\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] = grad_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Node):\n",
    "    def __init__(self, nodes, weights, bias, name='Linear'):\n",
    "        super().__init__(inputs=[nodes, weights, bias], name=name)\n",
    "        self.w_node = weights\n",
    "        self.x_node = nodes\n",
    "        self.b_node = bias\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"compute the wx + b using numpy\"\"\"\n",
    "        self.value = np.dot(self.x_node.value, self.w_node.value) + self.b_node.value\n",
    "\n",
    "    def backward(self):\n",
    "        for node in self.outputs:\n",
    "            # gradient_of_loss_of_this_output_node = node.gradient[self]\n",
    "            grad_cost = node.gradients[self]\n",
    "\n",
    "            self.gradients[self.w_node] = np.dot(self.x_node.value.T, grad_cost)\n",
    "            self.gradients[self.b_node] = np.sum(grad_cost * 1, axis=0, keepdims=False)\n",
    "            self.gradients[self.x_node] = np.dot(grad_cost, self.w_node.value.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Node):\n",
    "    def __init__(self, node, name='Sigmoid'):\n",
    "        super().__init__(inputs=[node], name=name)\n",
    "        self.x_node = node\n",
    "        self.partial = None\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1.0 / (1 + np.exp(-1 * x))\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = self._sigmoid(self.x_node.value)\n",
    "\n",
    "    def backward(self):\n",
    "        y = self.value\n",
    "        self.partial = y * (1 - y)\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self.x_node] = grad_cost * self.partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Node):\n",
    "    \"\"\" 均方差loss \"\"\"\n",
    "    def __init__(self, y_true, y_hat, name='MSE'):\n",
    "        super().__init__(inputs=[y_true, y_hat], name=name)\n",
    "        self.y_true_node = y_true\n",
    "        self.y_hat_node = y_hat\n",
    "        self.diff = None\n",
    "\n",
    "    def forward(self):\n",
    "        y_true_flatten = self.y_true_node.value.reshape(-1, 1)\n",
    "        y_hat_flatten = self.y_hat_node.value.reshape(-1, 1)\n",
    "        self.diff = y_true_flatten - y_hat_flatten\n",
    "        self.value = np.mean(self.diff ** 2)\n",
    "\n",
    "    def backward(self):\n",
    "        n = self.y_hat_node.value.shape[0]\n",
    "        self.gradients[self.y_true_node] = (2 / n) * self.diff\n",
    "        self.gradients[self.y_hat_node] = (-2 / n) * self.diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(Node):\n",
    "    \"\"\"\n",
    "    优化器基类\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target, name='Optimizer'):\n",
    "        super().__init__(inputs=[target], name=name)\n",
    "        self.target = target\n",
    "\n",
    "    def forward(self):\n",
    "        # for node in self.graph.nodes:\n",
    "        #     node.forward()\n",
    "        self.backward()\n",
    "\n",
    "    def backward(self):\n",
    "        for node in self.graph.nodes[-2::-1]:\n",
    "            node.backward()\n",
    "        self.update()\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        抽象方法，利用梯度更新可训练变量\n",
    "        \"\"\"\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(Optimizer):\n",
    "    \"\"\"\n",
    "    梯度下降优化器\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target, batch_size, learning_rate=0.01, name='GradientDescent'):\n",
    "        super().__init__(target, name=name)\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def update(self):\n",
    "        for node in self.graph.params:\n",
    "            gradient = node.gradients[node] / self.batch_size\n",
    "            node.value += -1 * self.learning_rate * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算图框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    \"\"\"\n",
    "    计算图类\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nodes = []  # 计算图内的节点的列表\n",
    "        self.params = []  # 变量节点\n",
    "\n",
    "    def add_node(self, node):\n",
    "        \"\"\"\n",
    "        添加节点\n",
    "        \"\"\"\n",
    "        self.nodes.append(node)\n",
    "        if isinstance(node, Variable):\n",
    "            self.params.append(node)\n",
    "\n",
    "    def clear_gradients(self):\n",
    "        \"\"\"\n",
    "        清除图中全部节点的雅可比矩阵\n",
    "        \"\"\"\n",
    "        for node in self.nodes:\n",
    "            node.clear_gradients()\n",
    "\n",
    "    def reset_value(self):\n",
    "        \"\"\"\n",
    "        重置图中全部节点的值\n",
    "        \"\"\"\n",
    "        for node in self.nodes:\n",
    "            node.reset_value(False)  # 每个节点不递归清除自己的子节点的值\n",
    "\n",
    "    def as_default(self):\n",
    "        global default_graph\n",
    "        default_graph = self\n",
    "\n",
    "\n",
    "# 全局默认计算图\n",
    "default_graph = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "class Session:\n",
    "\n",
    "    def __init__(self, graph=default_graph):\n",
    "        self.graph = graph\n",
    "        # self.topological_sort()\n",
    "\n",
    "    def topological_sort(self):\n",
    "        G = {}\n",
    "        nodes = [n for n in self.graph.nodes]\n",
    "        while len(nodes) > 0:\n",
    "            n = nodes.pop(0)\n",
    "            if n not in G:\n",
    "                G[n] = {'in': set(), 'out': set()}\n",
    "            for m in n.outputs:\n",
    "                if m not in G:\n",
    "                    G[m] = {'in': set(), 'out': set()}\n",
    "                G[n]['out'].add(m)\n",
    "                G[m]['in'].add(n)\n",
    "                nodes.append(m)\n",
    "\n",
    "        L = []\n",
    "        S = set(self.graph.nodes)\n",
    "        while len(S) > 0:\n",
    "            n = S.pop()\n",
    "            L.append(n)\n",
    "            for m in n.outputs:\n",
    "                G[n]['out'].remove(m)\n",
    "                G[m]['in'].remove(n)\n",
    "                # if no other incoming edges add to S\n",
    "                if len(G[m]['in']) == 0:\n",
    "                    S.add(m)\n",
    "\n",
    "        self.graph.nodes = L\n",
    "        return L\n",
    "\n",
    "    def eval(self, node):\n",
    "        for n in node.inputs:\n",
    "            self.eval(n)\n",
    "        node.forward()\n",
    "        return node.value\n",
    "\n",
    "    def run(self, fetches, feed_dict=None):\n",
    "        for k, v in feed_dict.items():\n",
    "            if isinstance(k, PlaceHolder):\n",
    "                k.value = v\n",
    "\n",
    "        fetches_result = []\n",
    "        for n in fetches:\n",
    "            if n.value is None:\n",
    "                fetches_result.append(self.eval(n))\n",
    "            else:\n",
    "                fetches_result.append(n.value)\n",
    "        return tuple(fetches_result)\n",
    "\n",
    "    @staticmethod\n",
    "    @contextmanager\n",
    "    def session(graph):\n",
    "        sess = Session(graph)\n",
    "        yield sess\n",
    "        graph.reset_value()\n",
    "        graph.clear_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal(shape, loc=0.0, scale=1.0):\n",
    "    return np.random.normal(loc=loc, scale=scale, size=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建计算图\n",
    "graph = Graph()\n",
    "graph.as_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化参数\n",
    "n_features = dataset.shape[1]  # X特征数\n",
    "n_hidden = 100  # 隐藏层个数\n",
    "X = PlaceHolder(name='X')\n",
    "y = PlaceHolder(name='y')\n",
    "W1 = Variable(normal((n_features, n_hidden), scale=1), name='W1')\n",
    "b1 = Variable(np.zeros(n_hidden), name='b1')\n",
    "W2 = Variable(normal((n_hidden, 1), scale=1), name='W2')\n",
    "b2 = Variable(np.zeros(1), name='b2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "linear_output = Linear(X, W1, b1, name='h1')\n",
    "sigmoid_output = Sigmoid(linear_output, name='h1_sig')\n",
    "yhat = Linear(sigmoid_output, W2, b2, name='yhat')\n",
    "loss = MSE(y, yhat, name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 3001\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建优化器\n",
    "optimizer = GradientDescent(loss, batch_size, learning_rate=0.001, name='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss = 541.138\n",
      "Epoch: 101, loss = 86.765\n",
      "Epoch: 201, loss = 72.227\n",
      "Epoch: 301, loss = 70.571\n",
      "Epoch: 401, loss = 69.982\n",
      "Epoch: 501, loss = 69.649\n",
      "Epoch: 601, loss = 69.272\n",
      "Epoch: 701, loss = 68.700\n",
      "Epoch: 801, loss = 68.400\n",
      "Epoch: 901, loss = 68.234\n",
      "Epoch: 1001, loss = 68.073\n",
      "Epoch: 1101, loss = 67.929\n",
      "Epoch: 1201, loss = 67.788\n",
      "Epoch: 1301, loss = 67.650\n",
      "Epoch: 1401, loss = 67.514\n",
      "Epoch: 1501, loss = 67.187\n",
      "Epoch: 1601, loss = 67.059\n",
      "Epoch: 1701, loss = 66.934\n",
      "Epoch: 1801, loss = 66.810\n",
      "Epoch: 1901, loss = 66.683\n",
      "Epoch: 2001, loss = 66.555\n",
      "Epoch: 2101, loss = 66.436\n",
      "Epoch: 2201, loss = 66.319\n",
      "Epoch: 2301, loss = 66.163\n",
      "Epoch: 2401, loss = 66.051\n",
      "Epoch: 2501, loss = 65.940\n",
      "Epoch: 2601, loss = 65.832\n",
      "Epoch: 2701, loss = 65.726\n",
      "Epoch: 2801, loss = 65.622\n",
      "Epoch: 2901, loss = 65.520\n",
      "Epoch: 3001, loss = 65.419\n"
     ]
    }
   ],
   "source": [
    "with Session.session(graph) as sess:\n",
    "    for n in range(epoch):\n",
    "        loss_sum = 0\n",
    "        n_step = len(dataset) // batch_size + 1\n",
    "        for i in range(n_step):\n",
    "            b = i * batch_size\n",
    "            e = b + batch_size\n",
    "            if e > len(dataset):\n",
    "                b = -batch_size\n",
    "                e = len(dataset)\n",
    "            batch_dataset = dataset[b:e]\n",
    "            batch_labels = labels[b:e]\n",
    "            _, los = sess.run([optimizer, loss], feed_dict={X: batch_dataset, y: batch_labels})\n",
    "            loss_sum += los\n",
    "        if n % 100 == 0:\n",
    "            print('Epoch: {}, loss = {:.3f}'.format(n+1, loss_sum/n_step))\n",
    "            losses.append(los)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 豆瓣评论分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_file = r'D:\\development\\jupyter\\datasource\\movie_comments.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\development\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "comment = pd.read_csv(comment_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2  3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3  4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4  5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = []\n",
    "stars = []\n",
    "for i in range(len(comment['star'])):\n",
    "    try:\n",
    "        stars.append(int(comment['star'][i]))\n",
    "        comments.append(comment['comment'][i])\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star = set()\n",
    "for i in stars:\n",
    "    star.add(i)\n",
    "star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    2\n",
       "3    4\n",
       "4    1\n",
       "5    1\n",
       "6    2\n",
       "7    4\n",
       "8    4\n",
       "9    1\n",
       "Name: star, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment['star'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stop_words(path):\n",
    "    stop_words = set()\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            stop_words.add(line.strip())\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = load_stop_words(r'D:\\development\\jupyter\\corpus\\stop_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(string):\n",
    "    return ' '.join(re.findall(r'[\\d|\\w]+', string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(text):\n",
    "    return [w for w in jieba.lcut(token(text)) if len(w.strip()) > 0 and w not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮番上场，视物理逻辑于不顾，不得不说有钱真好，随意胡闹'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = comment['comment'][1]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\noone\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.608 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['首映礼',\n",
       " '太',\n",
       " '恐怖',\n",
       " '电影',\n",
       " '不讲道理',\n",
       " '吴京',\n",
       " '粉红',\n",
       " '英雄',\n",
       " '梦',\n",
       " '装备',\n",
       " '轮番',\n",
       " '上场',\n",
       " '视',\n",
       " '物理',\n",
       " '逻辑',\n",
       " '不顾',\n",
       " '说',\n",
       " '有钱',\n",
       " '真',\n",
       " '随意',\n",
       " '胡闹']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = cut(token(t))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec.load(r'D:\\development\\jupyter\\corpus\\zhwiki_sougou_news_word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vec(sentence):\n",
    "    return np.mean([word2vec.wv[w] for w in cut(sentence) if w in word2vec.wv], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.5, 3.5, 4.5])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([[1,2,3],[4,5,6]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.4314901 , -0.21323907,  1.8681071 ,  0.08058092,  0.5520421 ,\n",
       "        0.94416606,  0.16812155, -0.5000285 , -0.8309715 , -0.9423348 ,\n",
       "        0.12564968, -0.662259  ,  0.681413  , -0.4508646 ,  0.6692805 ,\n",
       "       -0.9565575 , -0.3335797 , -1.327392  ,  0.9559461 , -0.9573267 ,\n",
       "        1.3100893 ,  0.4411884 ,  0.11875989,  0.02037793,  0.6158737 ,\n",
       "       -0.73443556, -0.8963304 ,  1.462498  ,  0.01595178,  0.45038795,\n",
       "        1.0694367 , -0.6588845 , -0.56813586, -0.40205342,  0.27342457,\n",
       "       -0.36333033, -0.3247346 ,  0.7777479 ,  0.10208637,  0.83331925,\n",
       "        0.16834512, -0.18541825, -0.530091  ,  0.14174439, -0.441872  ,\n",
       "        0.41390347,  0.21868041,  0.8984635 ,  0.50942695, -0.42627645,\n",
       "       -1.1100296 , -0.11748734, -1.0356447 , -1.22351   , -0.00699306,\n",
       "        1.7741734 , -0.64081633,  0.40560696,  1.4500227 , -0.01240737,\n",
       "        1.2645762 , -0.06243144, -0.360958  , -0.37007433,  0.3438284 ,\n",
       "        0.13069817, -0.26574332, -0.17688648, -0.22753856, -1.267106  ,\n",
       "        0.53771126, -0.6961406 ,  0.1631657 ,  1.0532548 , -0.15809627,\n",
       "        1.3245733 ,  1.3743098 , -0.4477293 ,  0.8957669 ,  0.1840019 ,\n",
       "       -0.83338803,  0.07800856,  0.3575894 ,  0.01160158, -0.7405318 ,\n",
       "        0.6367809 ,  1.266562  ,  0.43800187,  1.0269845 , -0.6336646 ,\n",
       "        0.5328878 , -0.459705  , -0.16497858,  0.26396057, -1.1179649 ,\n",
       "       -0.05335836, -0.54722035,  0.2610099 ,  0.5927758 , -0.22406682],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2vec(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = [sentence2vec(i) for i in comments[:10000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-139-61894c065cc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "np.isnan(dataset2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义句子向量模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用普林斯顿的句子向量论文A SIMPLE BUT TOUGH-TO-BEAT BASELINE FOR SENTENCE EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "def get_weighted_average(We, x, w):\n",
    "    \"\"\"\n",
    "    Compute the weighted average vectors\n",
    "    :param We: We[i,:] is the vector for word i\n",
    "    :param x: x[i, :] are the indices of the words in sentence i\n",
    "    :param w: w[i, :] are the weights for the words in sentence i\n",
    "    :return: emb[i, :] are the weighted average vector for sentence i\n",
    "    \"\"\"\n",
    "    n_samples = x.shape[0]\n",
    "    emb = np.zeros((n_samples, We.shape[1]))\n",
    "    for i in range(n_samples):\n",
    "        emb[i, :] = w[i, :].dot(We[x[i, :], :]) / np.count_nonzero(w[i, :])\n",
    "    return emb\n",
    "\n",
    "\n",
    "def compute_pc(X, npc=1):\n",
    "    \"\"\"\n",
    "    Compute the principal components. DO NOT MAKE THE DATA ZERO MEAN!\n",
    "    :param X: X[i,:] is a data point\n",
    "    :param npc: number of principal components to remove\n",
    "    :return: component_[i,:] is the i-th pc\n",
    "    \"\"\"\n",
    "    svd = TruncatedSVD(n_components=npc, n_iter=7, random_state=0)\n",
    "    svd.fit(X)\n",
    "    return svd.components_\n",
    "\n",
    "\n",
    "def remove_pc(X, npc=1):\n",
    "    \"\"\"\n",
    "    Remove the projection on the principal components\n",
    "    :param X: X[i,:] is a data point\n",
    "    :param npc: number of principal components to remove\n",
    "    :return: XX[i, :] is the data point after removing its projection\n",
    "    \"\"\"\n",
    "    pc = compute_pc(X, npc)\n",
    "    if npc == 1:\n",
    "        XX = X - X.dot(pc.transpose()) * pc\n",
    "    else:\n",
    "        XX = X - X.dot(pc.transpose()).dot(pc)\n",
    "    return XX\n",
    "\n",
    "\n",
    "def SIF_embedding(We, x, w, params):\n",
    "    \"\"\"\n",
    "    Compute the scores between pairs of sentences using weighted average + removing the projection on the first principal component\n",
    "    :param We: We[i,:] is the vector for word i\n",
    "    :param x: x[i, :] are the indices of the words in the i-th sentence\n",
    "    :param w: w[i, :] are the weights for the words in the i-th sentence\n",
    "    :param params.rmpc: if >0, remove the projections of the sentence embeddings to their first principal component\n",
    "    :return: emb, emb[i, :] is the embedding for sentence i\n",
    "    \"\"\"\n",
    "    emb = get_weighted_average(We, x, w)\n",
    "    if params.rmpc > 0:\n",
    "        emb = remove_pc(emb, params.rmpc)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def getWordWeight(word_frequency, a=1e-3):\n",
    "    if a <= 0:  # when the parameter makes no sense, use unweighted\n",
    "        a = 1.0\n",
    "    word2weight = {}\n",
    "    N = 0\n",
    "    for w, f in word_frequency.items():\n",
    "        word2weight[w] = f\n",
    "        N += f\n",
    "    for key, value in word2weight.items():\n",
    "        word2weight[key] = a / (a + value / N)\n",
    "    return word2weight\n",
    "\n",
    "\n",
    "def sentences2idx(sentences, words):\n",
    "    \"\"\"\n",
    "    Given a list of sentences, output array of word indices that can be fed into the algorithms.\n",
    "    :param sentences: a list of sentences\n",
    "    :param words: a dictionary, words['str'] is the indices of the word 'str'\n",
    "    :return: x1, m1. x1[i, :] is the word indices in sentence i, m1[i,:] is the mask for sentence i (0 means no word at the location)\n",
    "    \"\"\"\n",
    "    seq1 = []\n",
    "    for s in sentences:\n",
    "        seq1.append(getSeq(s, words))\n",
    "    x1, m1 = prepare_data(seq1)\n",
    "    return x1, m1\n",
    "\n",
    "\n",
    "def prepare_data(list_of_seqs):\n",
    "    lengths = [len(s) for s in list_of_seqs]\n",
    "    n_samples = len(list_of_seqs)\n",
    "    maxlen = np.max(lengths)\n",
    "    x = np.zeros((n_samples, maxlen)).astype('int32')\n",
    "    x_mask = np.zeros((n_samples, maxlen)).astype('float32')\n",
    "    for idx, s in enumerate(list_of_seqs):\n",
    "        x[idx, :lengths[idx]] = s\n",
    "        x_mask[idx, :lengths[idx]] = 1.\n",
    "    x_mask = np.asarray(x_mask, dtype='float32')\n",
    "    return x, x_mask\n",
    "\n",
    "\n",
    "def getSeq(s, words_map):\n",
    "    idx = []\n",
    "    for w in s:\n",
    "        if w in words_map:\n",
    "            idx.append(words_map[w])\n",
    "        else:\n",
    "            idx.append(len(words_map) - 1)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def getWeight(words, word2weight):\n",
    "    weight4ind = {}\n",
    "    for word, ind in words.items():\n",
    "        if word in word2weight:\n",
    "            weight4ind[ind] = word2weight[word]\n",
    "        else:\n",
    "            weight4ind[ind] = 1.0\n",
    "    return weight4ind\n",
    "\n",
    "\n",
    "def seq2weight(seq, mask, weight4ind):\n",
    "    weight = np.zeros(seq.shape).astype('float32')\n",
    "    for i in range(seq.shape[0]):\n",
    "        for j in range(seq.shape[1]):\n",
    "            if mask[i, j] > 0 and seq[i, j] >= 0:\n",
    "                weight[i, j] = weight4ind[seq[i, j]]\n",
    "    weight = np.asarray(weight, dtype='float32')\n",
    "    return weight\n",
    "\n",
    "\n",
    "class params(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.LW = 1e-5\n",
    "        self.LC = 1e-5\n",
    "        self.eta = 0.05\n",
    "\n",
    "    def __str__(self):\n",
    "        t = \"LW\", self.LW, \", LC\", self.LC, \", eta\", self.eta\n",
    "        t = map(str, t)\n",
    "        return ' '.join(t)\n",
    "\n",
    "\n",
    "class SIFModel:\n",
    "    def __init__(self, word2vec_model, weightpara=1e-3):\n",
    "        \"\"\"\n",
    "        :param word2vec_model: word2vec模型\n",
    "        :param stop_words: 停用词\n",
    "        :param weightpara:\n",
    "        \"\"\"\n",
    "        self.word2vec_model = word2vec_model\n",
    "        words = word2vec_model.wv.index2word\n",
    "        self.word_vectors = word2vec_model.wv.vectors\n",
    "        word_frequency = {w: v.count for w, v in word2vec_model.wv.vocab.items()}\n",
    "        self.word_index_map = {w: n for n, w in enumerate(words)}\n",
    "        word2weight = getWordWeight(word_frequency, weightpara)\n",
    "        self.weight4ind = getWeight(self.word_index_map, word2weight)\n",
    "\n",
    "    def sentence2vec(self, sentences):\n",
    "        \"\"\"\n",
    "        计算句子向量\n",
    "        :param sentences:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sens = [cut(s) for s in sentences]\n",
    "        x, m = sentences2idx(sens, self.word_index_map)\n",
    "        w = seq2weight(x, m, self.weight4ind)\n",
    "        p = params()\n",
    "        p.rmpc = 0 if len(sentences) <= 1 else 1\n",
    "        return SIF_embedding(self.word_vectors, x, w, p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建模型\n",
    "sif_model = SIFModel(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 100),\n",
       " array([[-0.37148577, -0.14582501,  1.61984146,  0.07227427,  0.52611715,\n",
       "          0.79112673,  0.15752387, -0.40599465, -0.79713565, -0.8083415 ,\n",
       "          0.08923913, -0.62931818,  0.60452098, -0.24124436,  0.66816014,\n",
       "         -1.01637352, -0.36557609, -1.32418454,  0.83325166, -0.73187137,\n",
       "          1.06585884,  0.40391096,  0.11407755,  0.08116055,  0.63171309,\n",
       "         -0.565512  , -0.84652507,  1.29514861,  0.01530122,  0.47093913,\n",
       "          0.95852309, -0.57534248, -0.4818776 , -0.30580068,  0.1150021 ,\n",
       "         -0.34451789, -0.20868465,  0.63513392,  0.02345112,  0.86482495,\n",
       "          0.09472033, -0.19055696, -0.47505215,  0.23752721, -0.37120318,\n",
       "          0.41446254,  0.18967609,  0.68884474,  0.40239415, -0.30909076,\n",
       "         -0.88811129, -0.13006161, -0.81700778, -1.11770785,  0.09335148,\n",
       "          1.65184915, -0.47939846,  0.43245053,  1.2934233 , -0.025197  ,\n",
       "          1.20240521, -0.05018903, -0.31783891, -0.3727361 ,  0.3252449 ,\n",
       "          0.05112781, -0.25066045, -0.18854585, -0.21965767, -1.19547057,\n",
       "          0.41887119, -0.56918257,  0.10705295,  0.87859082, -0.07804767,\n",
       "          1.16508389,  1.18367851, -0.40988749,  0.82371366,  0.08879372,\n",
       "         -0.73152906,  0.08550815,  0.41372603,  0.03473616, -0.68736064,\n",
       "          0.6375066 ,  1.0901767 ,  0.3864713 ,  0.9366129 , -0.48942074,\n",
       "          0.54992163, -0.391918  , -0.08168498,  0.22358182, -0.97959018,\n",
       "          0.04849971, -0.48889306,  0.22752939,  0.43288526, -0.12990345]]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2v = sif_model.sentence2vec([t])\n",
    "s2v.shape, s2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2v[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\development\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "dataset = np.array([sif_model.sentence2vec([i])[0] for i in comments[:10000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0:100, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08239317, -0.42906678,  0.79234445,  0.02816611,  0.62318861,\n",
       "        1.21528304, -0.50583208, -0.05945163, -0.83206856,  0.2146053 ,\n",
       "        1.5383215 ,  0.09208171,  1.47147059,  0.08726862,  0.38526055,\n",
       "       -0.07311188, -1.01709723,  0.31261376,  1.20884979, -1.48534298,\n",
       "        2.40975857,  0.74284488, -0.39396742,  0.07520241, -0.70188618,\n",
       "       -0.54689884,  0.3483099 ,  0.65707636,  0.44856936, -0.05704463,\n",
       "        1.43716204, -0.72094291, -0.81099075,  0.37981534, -0.18492876,\n",
       "        0.12555869, -0.47824076,  0.37308496,  0.52149045, -0.38853732,\n",
       "       -0.29278687,  0.29526719, -1.55026436, -0.8495968 ,  0.1184432 ,\n",
       "        0.38903669,  0.51281732,  1.38472247,  0.40694526, -1.6350652 ,\n",
       "       -0.0486272 , -0.08006042, -0.55214089, -0.58384514, -0.38205394,\n",
       "        1.39344573, -0.23462686, -0.48398241,  1.03827929,  0.75543386,\n",
       "        1.68546772,  0.59013867,  0.11416898,  0.09117563, -1.57665789,\n",
       "       -1.05298233,  0.16447324, -1.41008735, -0.45793965, -1.05336249,\n",
       "        0.76167798, -0.94914049,  0.17357141, -0.95546716,  0.10129005,\n",
       "       -0.15027091,  0.60723191,  0.83899009, -0.37988514, -0.25839743,\n",
       "       -1.06469023,  1.81042993, -0.44241196, -0.39523059, -0.5990935 ,\n",
       "       -0.71896911,  0.76976281,  0.96182662,  0.61456603, -0.4069213 ,\n",
       "       -0.22070466,  0.3391147 , -0.07172307,  0.35913387, -1.67620182,\n",
       "       -0.19047202, -0.74263674,  0.66940904,  0.05345455, -0.66406745])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(stars[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (np.arange(5) == labels[:,None]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 5)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0:100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8900"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(dataset).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(df.fillna(0).values).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.fillna(0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feature = 100\n",
    "num_labels = 5\n",
    "num_nodes= 1024\n",
    "batch_size = 100\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, n_feature))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "\n",
    "    # Variables.\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([n_feature, num_nodes]))\n",
    "    biases_1 = tf.Variable(tf.zeros([num_nodes]))\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([num_nodes, num_labels]))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1) # add relu layer\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    softmax = tf.nn.softmax(logits_2)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits_2, labels = tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "\n",
    "    # Predictions for the training\n",
    "    train_prediction = tf.nn.softmax(logits_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (labels.shape[0] - batch_size)\n",
    "        batch_data = dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 100 == 0):\n",
    "            print(\"Minibatch loss at step {}: loss={}, accuracy={}\".format(step, l, accuracy(predictions, batch_labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
